---
title: "MovieLens Report"
author: "Syed Reyadh"
output:
  pdf_document: default
  word_document: default
---

###Introduction 

A recommender system or a recommendation system is a subclass of information filtering system that seeks to predict the "rating" or "preference" a user would give to an item. Recommendation systems use ratings that users have given items to make specific recommendations. Companies that sell many products to many customers and permit these customers to rate their products, use customers rating to predict their preferences or rating for another item. Netflix uses a recommendation system to predict if user rating for specific movies. motivated by some of the approaches taken by the winners of the Netflix challenges, On October 2006, Netflix offered a challenge to the data science community: improve our recommendation algorithm by 10% and win a million dollars. In September 2009, the winners were announced. You can read a good summary of how the winning algorithm was put together here and a more detailed explanation here. We will now show you some of the data analysis strategies used by the winning team.

this assignment is to accomplish a similar goal which is to build a recommendation system that recommends movies based on a rating scale.

##Data set 
for this project the MovieLens Data set collected by GroupLens Research and can be found in MovieLens web site (http://movielens.org).

##Data Loading
the data set is loaded using the code provided by course instucture in this link https://bit.ly/2Ng6tVW which split the data into edx set and 10% validation set.
the edx set will be split into training and test set,and validation set will be used to final evaluation.
```{r }
#############################################################
# Create edx set, validation set, and submission file
#############################################################

# Note: this process could take a couple of minutes

if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")

# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip

dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- read.table(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                      col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(levels(movieId))[movieId],
                                           title = as.character(title),
                                           genres = as.character(genres))

movielens <- left_join(ratings, movies, by = "movieId")

# Validation set will be 10% of MovieLens data

set.seed(1)
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,] 

# Make sure userId and movieId in validation set are also in edx set
#validation set 

validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

# Add rows removed from validation set back into edx set

removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)

###################################################################################
#################################################
```
before the analysis we check for any NA value 
```{r}
anyNA(edx)
```

## Data Summary and Explortory Data Analysis 
after loading the data set we start by looking at the data structure and type we can see that there is six variable (userId,movieID,rating,timestamp,title,genres).as shown the year need to be seperated from title if needed for prediction also the genres 
need sepration if needed.
```{r}
str(edx)
summary(edx)
```
from the summary of data we see that the minimum rating is 1 and max is 5 and the mean for the rating is 3.512 and the mode is 4.0.
```{r echo=FALSE}
edx %>% group_by(rating) %>% summarize(count = n()) %>% top_n(5) %>%
	arrange(desc(count))  
```
this code prints the number of unique movies and users in the data set:
```{r echo= FALSE}
edx %>% 
  summarize(n_users = n_distinct(userId),
            n_movies = n_distinct(movieId))

```

to see how the number of ratings for every movie, we do that by plotting histogram 

```{r  echo=FALSE}
edx %>% count(movieId) %>% ggplot(aes(n))+
  geom_histogram(color = "black" , fill= "light blue",bins = 30 , binwidth = 0.2)+
  scale_x_log10()+
  ggtitle(" number of Rating per Movie")+
  theme_gray()
```

We note that some movies get more ratings it could be due to popularity. Now we visualize the number of ratings for each user 

```{r echo= FALSE }
edx %>% count(userId) %>% ggplot(aes(n))+
  geom_histogram(color = "black" , fill= "light blue" , bins = 30, binwidth = 0.2)+
  ggtitle(" Number of Rating Per User")+
  scale_x_log10()+
  theme_gray()

```
 
we see that some user are active more than others at rating movies.

Now let's plot the rating for each movie genre 

```{r echo=FALSE }
edx %>% separate_rows(genres, sep = "\\|") %>%
  group_by(genres) %>%
  summarize(count = n()) %>%
  arrange(desc(count)) %>% ggplot(aes(genres,count)) + 
  geom_bar(aes(fill =genres),stat = "identity")+ 
  labs(title = " Number of Rating for Each Genre")+
  theme(axis.text.x  = element_text(angle= 90, vjust = 50 ))+
  theme_light()
 
```

let's see the top 10 most popular genre 

```{r echo= FALSE }
edx %>% separate_rows(genres, sep = "\\|") %>%
	group_by(genres) %>%
	summarize(count = n()) %>%
	arrange(desc(count))
```
  
  
##Data Partitioning  
before building the model we partition the edx data set into 20% for test set and 80% for the training set.
```{r}
set.seed(1)
test_index <- createDataPartition(y = edx$rating, times = 1, p = 0.2, list = FALSE)
train_set <- edx[-test_index,]
test_set <- edx[test_index,]
```


## Model building and RMSE calculation 
The Netflix challenge used typical error loss. They decided on a winner based on the residual mean squared error (RMSE) on a test set. The RMSE will be the measure of accuracy.
```{r}

RMSE <- function(true_ratings, predicted_ratings){
  sqrt(mean((true_ratings - predicted_ratings)^2, na.rm = TRUE))
}
```

###First Model
In the first model, we predict the same rating for all movies regardless of the user. a model that assumes the same rating for all movies and users. no bias are considered here. this method assumes the following linear equation is true: 
$Y~u,i~ = ?? + ??~u,i~$

```{r}
Mu_1 <- mean(train_set$rating)
Mu_1

```

```{r}
naive_rmse <- RMSE(test_set$rating,Mu_1)
naive_rmse
```

this code creates a table for the RMSE result to store all the result from each method to compare.
```{r}
rmse_results <- data_frame(method = "Just the average", RMSE = naive_rmse)
rmse_results%>% knitr::kable()
```

###Second Model| Movie Effect
 As we saw on the exploratory analysis some movies are rated more than other we can augment our previous model by adding the term  $b~i$ to represent the average ranking for movie $i$ We can again use least squared to estimate considering the movie bias, in statics they refer to $b$ as effect but in the Netflix paper referred them as "Bias"
$Y~u,i~ = ?? + b~i~ + ??~u,i~$
Because there are thousands $b~i$, each movie gets one, the lm() function will be very slow here. so we compute it using the average this way :
```{r}
Mu_2 <- mean(train_set$rating) 
movie_avgs <- train_set %>% 
  group_by(movieId) %>% 
  summarize(b_i = mean(rating - Mu_2))
```

we can see that variability in the estimate as plotted here 

```{r echo=FALSE}
movie_avgs %>% qplot(b_i, geom ="histogram", bins = 10, data = ., color = I("black"))

```

let's see how the prediction improves after altering  the equation $Y~u,i~ = ?? + b~i$
```{r}
predicted_ratings <- Mu_2 + test_set %>% 
  left_join(movie_avgs, by='movieId') %>%
  pull(b_i)

model_2_rmse <- RMSE(predicted_ratings, test_set$rating)
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Movie Effect Model",  
                                     RMSE = model_2_rmse))
rmse_results %>% knitr::kable()
                
```

###Third Model| User Effect 
let's compure the user $u$ for , for those who rated over 100 movies.

```{r echo= FALSE}
train_set %>% 
  group_by(userId) %>% 
  summarize(b_u = mean(rating)) %>% 
  filter(n()>=100) %>%
  ggplot(aes(b_u)) + 
  geom_histogram(bins = 30, color = "black")
```

Notice that there is substantial variability across users ratings as well. This implies that a further improvement to our model may be $Y~u,i~ = ?? + b~i~ + ??~u,i~$ we could fit this model by using use the lm() function but as mentioned earlier it would be very slow $lm(rating ~ as.factor(movieId) + as.factor(userId))$
so here is the code 
```{r}
user_avgs <- train_set %>% 
  left_join(movie_avgs, by='movieId') %>%
  group_by(userId) %>%
  summarize(b_u = mean(rating - Mu_2 - b_i))
  
```
now let's see how RMSE improved this time 
```{r}
predicted_ratings <- test_set %>% 
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  mutate(pred = Mu_2 + b_i + b_u) %>%
  pull(pred)


model_3_rmse <- RMSE(predicted_ratings, test_set$rating)
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Movie + User Effects Model",  
                                     RMSE = model_3_rmse))
rmse_results%>% knitr::kable()

```

## RMSE of the validation set

```{r}
valid_pred_rating <- validation %>%
  left_join(movie_avgs , by = "movieId" ) %>% 
  left_join(user_avgs , by = "userId") %>%
  mutate(pred = Mu_2 + b_i + b_u ) %>%
  pull(pred)

model_3_valid <- RMSE(validation$rating, valid_pred_rating)
rmse_results <-  bind_rows( rmse_results, data_frame(Method = "Validation Results" , RMSE = model_3_valid))
rmse_results%>% knitr::kable()
```

## Conclusion
I have developed a naive approach, movie effect and user+movie effect the best RMSE given by the third model. for further analysis more complicated prediction using the release year of the movie as a bias considering old movies such as the 60 or 80 periods as another genre for a better predicting model. a linear model for precision is recommended.
